{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Notebook describe how to load DeepECG-SSL and use it on your personal training pipeline\n",
    "\n",
    "- Make sure you deployed [`Fairseq-signals`](https://github.com/HeartWise-AI/fairseq-signals) locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "\n",
    "Inference can be done either from command line interface or from this notebook\n",
    "\n",
    "#### Inference in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import useful old code\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import Any, Dict, Optional, Union\n",
    "\n",
    "#TODO: configure it\n",
    "project_dir = ##INIT_TO_FAIRSEQ_SIGNAL_PATH\n",
    "root_dir = project_dir\n",
    "if not root_dir in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"checkpoint_utils\", f\"{project_dir}/fairseq_signals/utils/checkpoint_utils.py\")\n",
    "checkpoint_utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(checkpoint_utils)\n",
    "\n",
    "\n",
    "class WCREcgTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_path: str,\n",
    "        pretrained_path: str = None,\n",
    "        overrides: Optional[Dict[str, Any]] = None,\n",
    "        task=None,\n",
    "        strict=True,\n",
    "        suffix=\"\",\n",
    "        num_shards=1,\n",
    "        state=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        overrides = {} if overrides is None else vars(overrides)\n",
    "        if pretrained_path is not None:\n",
    "            overrides.update({\"model_path\": pretrained_path})\n",
    "        model, saved_cfg, task = checkpoint_utils.load_model_and_task(\n",
    "            model_path,\n",
    "            arg_overrides=overrides,\n",
    "            suffix=suffix\n",
    "        )\n",
    "\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x, padding_mask=None):\n",
    "        net_input = { \"source\": x, \"padding_mask\": padding_mask}\n",
    "        net_output = self.model(**net_input)\n",
    "        return self.model.get_logits(net_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "m_root = '/media/data1/achilsowa/results/fairseq/outputs/'\n",
    "\n",
    "m_paths = {\n",
    "    \"SSL\": '', #SSL_PATH\n",
    "    \"FT_AFIB-5\": '', #SSL_PATH\n",
    "    \"FT_LABELS-77\": os.path.join(m_root, \"2024-10-08/04-39-01/checkpoint_last-ft-labels-77-bce/checkpoint_best.pt\")\n",
    "}\n",
    "\n",
    "# Get the path to the root directory of your project\n",
    "root_path = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))  # Adjust according to your folder depth\n",
    "\n",
    "# Add the root directory to sys.path\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "from models.modules.wcr import WCREcgTransformer\n",
    "\n",
    "\n",
    "model_ssl = WCREcgTransformer(m_paths['FT_AFIB'], m_paths['SSL'])\n",
    "model_fevg = WCREcgTransformer(m_paths['FT_FEVG-REG'], m_paths['SSL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. download ptb\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def generate_train_cli(\n",
    "    devices=1,\n",
    "    encoder='_last',\n",
    "    task = 'labels-77',\n",
    "    num_labels = 77,\n",
    "    mode='ft', # possible values are 'ft', 'le', 'e2e'\n",
    "    is_df=True,\n",
    "    cls='',\n",
    "    wd=0,\n",
    "    criterion='binary_cross_entropy_with_logits'\n",
    "):    \n",
    "    def loss_str():\n",
    "        if criterion == 'asymmetric':\n",
    "            return 'as'\n",
    "        if criterion == 'binary_focal':\n",
    "            return 'bf'\n",
    "        if criterion == 'mse':\n",
    "            return 'mse'\n",
    "        if criterion == 'binary_cross_entropy_with_logits':\n",
    "            return 'bce'\n",
    "        if criterion == 'mlsml':\n",
    "            return 'mlsml'\n",
    "        assert False, 'Invalid error'\n",
    "    \n",
    "    cli = f'CUDA_VISIBLE_DEVICES={devices} fairseq-hydra-train '\n",
    "    if criterion == 'mse':\n",
    "        cli += f'common.fp16=false '\n",
    "    else:\n",
    "        cli +=f'common.fp16=true '\n",
    "    cli += f'task.data=/media/data1/achilsowa/datasets/fairseq/mhi-mimic-code15/manifest/finetune/{task} '\n",
    "    if mode == 'e2e':\n",
    "        cli += f'model.no_pretrained_weights=true '\n",
    "        encoder = '_e2e'\n",
    "    else:\n",
    "        cli += f'model.model_path=/media/data1/achilsowa/results/fairseq/outputs/2024-09-22/03-16-32/checkpoints-all/checkpoint{encoder}.pt '\n",
    "    if cls == 'attn':\n",
    "        cli += f'model._name=ecg_transformer_attn_classifier '\n",
    "    if wd:\n",
    "        cli += f'optimizer.weight_decay={wd} '\n",
    "    if mode == 'le':\n",
    "        cli += f'model.linear_evaluation=true '\n",
    "    \n",
    "    if is_df:\n",
    "        cli += f'+task.df_dataset=true '\n",
    "    else:\n",
    "        cli += f'+task.npy_dataset=true '\n",
    "\n",
    "    cli += f'model.num_labels={num_labels} '\n",
    "    cli += f'criterion._name={criterion} '\n",
    "    if cls == 'attn':\n",
    "        cls = '-attn'\n",
    "    cli += f'checkpoint.save_dir=checkpoint{encoder}-{mode}-{task}-{loss_str()}{cls} '\n",
    "    cli += '--config-dir examples/w2v_cmsc/config/finetuning/ecg_transformer --config-name diagnosis'\n",
    "\n",
    "    return cli"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
